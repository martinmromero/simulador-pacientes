# Integraci√≥n con Ollama - Gu√≠a de Configuraci√≥n

## üéØ Configuraci√≥n Completada

La aplicaci√≥n ahora est√° configurada para usar Ollama en tu servidor de intranet.

### Configuraci√≥n actual:
- **IP**: 192.168.12.236
- **Puerto**: 11434
- **Modelo**: llama2 (configurable en .env)

---

## üöÄ Inicio R√°pido

### 1. Probar conexi√≥n con Ollama

```bash
cd backend
node test-ollama.js
```

Este script verificar√°:
- ‚úÖ Conectividad con el servidor Ollama
- ‚úÖ Modelos disponibles
- ‚úÖ Generaci√≥n de respuesta de prueba
- ‚úÖ Tiempo de respuesta

### 2. Iniciar el backend

```bash
cd backend
npm start
```

### 3. Verificar el health check

Abr√≠ en tu navegador:
```
http://localhost:3000/api/ia/health
```

Deber√≠as ver algo como:
```json
{
  "status": "ok",
  "ollama_conectado": true,
  "ollama_url": "192.168.12.236:11434",
  "modelo_configurado": "llama2",
  "modelos_disponibles": [
    { "name": "llama2:latest" },
    { "name": "mistral:latest" }
  ]
}
```

---

## ‚öôÔ∏è Configuraci√≥n (archivo .env)

El archivo `backend/.env` contiene:

```bash
# Configuraci√≥n de Ollama (Intranet)
OLLAMA_HOST=192.168.12.236
OLLAMA_PORT=11434
OLLAMA_MODEL=llama2

# Par√°metros de generaci√≥n de IA
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=150
AI_TOP_P=0.9
```

### Modelos recomendados:
- `llama2` - Modelo general, buen equilibrio
- `llama2:13b` - M√°s preciso pero m√°s lento
- `mistral` - R√°pido y eficiente
- `neural-chat` - Optimizado para conversaciones

Para cambiar el modelo, edit√° `OLLAMA_MODEL` en el .env y reinici√° el backend.

---

## üîß Cambios Realizados

### 1. backend/package.json
- ‚úÖ Agregada dependencia `axios` para HTTP requests

### 2. backend/.env
- ‚úÖ Agregadas variables de configuraci√≥n de Ollama
- ‚úÖ Configurado host: 192.168.12.236
- ‚úÖ Puerto: 11434
- ‚úÖ Modelo por defecto: llama2

### 3. backend/routes/ia.js
- ‚úÖ Implementada funci√≥n `llamarOllama()` para conectarse al servidor
- ‚úÖ Modificado endpoint `/generar-respuesta` para usar Ollama
- ‚úÖ Agregado fallback a respuestas simuladas si Ollama no responde
- ‚úÖ Agregado endpoint `/health` para verificar conectividad
- ‚úÖ Logging de tiempos de respuesta

---

## üìä Funcionamiento

### Flujo de generaci√≥n de respuesta:

1. **Frontend** env√≠a pregunta del estudiante ‚Üí `POST /api/ia/generar-respuesta`
2. **Backend** obtiene:
   - Caso cl√≠nico de la BD
   - Historial de conversaci√≥n
3. **Backend** construye prompt completo con contexto
4. **Backend** env√≠a prompt a Ollama (192.168.12.236:11434)
5. **Ollama** genera respuesta usando el modelo configurado
6. **Backend** devuelve respuesta + metadata (tiempo, tokens, etc.)
7. **Frontend** muestra respuesta del paciente

### Fallback inteligente:
- Si Ollama no responde ‚Üí usa respuestas simuladas
- Logs claros indican cuando est√° en modo fallback
- La app nunca se rompe por falta de conexi√≥n

---

## üêõ Troubleshooting

### Error: "ECONNREFUSED"
```
‚ùå No se puede conectar a Ollama
```

**Soluciones:**
1. Verific√° que Ollama est√© corriendo en el servidor:
   ```bash
   ssh usuario@192.168.12.236
   ollama list
   ```

2. Verific√° conectividad de red:
   ```bash
   ping 192.168.12.236
   curl http://192.168.12.236:11434/api/tags
   ```

3. Verific√° firewall en el servidor:
   ```bash
   # En el servidor Ollama
   sudo ufw allow 11434
   ```

### Error: "Model not found"
```
‚ùå El modelo "llama2" no est√° disponible
```

**Soluci√≥n:**
En el servidor Ollama, descarg√° el modelo:
```bash
ollama pull llama2
# o
ollama pull mistral
```

### Respuestas muy lentas (> 10 segundos)
```
‚ö†Ô∏è Tiempo de respuesta alto
```

**Soluciones:**
1. Us√° un modelo m√°s peque√±o en .env:
   ```bash
   OLLAMA_MODEL=mistral
   ```

2. Reduc√≠ `AI_MAX_TOKENS`:
   ```bash
   AI_MAX_TOKENS=100
   ```

3. Verific√° carga del servidor:
   ```bash
   ssh usuario@192.168.12.236
   nvidia-smi  # Ver uso de GPU
   htop        # Ver uso de CPU
   ```

### Backend usa respuestas simuladas
```
‚ö†Ô∏è [IA] Ollama no disponible, usando respuestas simuladas
```

Esto es normal si:
- Ollama est√° temporalmente ca√≠do
- Hay problemas de red
- El modelo est√° ocupado

La app funciona igual pero con respuestas gen√©ricas.

---

## üìà Monitoreo

### Ver logs en tiempo real:
```bash
cd backend
npm start
```

Ver√°s logs como:
```
[IA] Configurado para usar Ollama en 192.168.12.236:11434 con modelo llama2
[IA] Session: abc-123 | Modelo: llama2 | Tiempo: 1250ms
[IA] Session: def-456 | Modelo: llama2 | Tiempo: 980ms
```

### M√©tricas importantes:
- ‚è±Ô∏è **Tiempo de respuesta**: Ideal < 3 segundos
- üéØ **Tasa de √©xito**: % de veces que Ollama responde vs fallback
- üìä **Tokens generados**: Cantidad de texto generado

---

## üéØ Pr√≥ximos Pasos

1. ‚úÖ **Prob√° la conexi√≥n**: `node test-ollama.js`
2. ‚úÖ **Inici√° el backend**: `npm start`
3. ‚úÖ **Verific√° health**: `http://localhost:3000/api/ia/health`
4. ‚úÖ **Prob√° desde el frontend**: Inici√° una sesi√≥n con un paciente
5. üìä **Monitore√° rendimiento**: Registr√° tiempos de respuesta
6. üîß **Optimiz√° si es necesario**: Ajust√° modelo o par√°metros

---

## üîê Consideraciones de Seguridad

- ‚ÑπÔ∏è Ollama en intranet (192.168.12.236) = Sin autenticaci√≥n
- ‚úÖ OK para ambiente de desarrollo/universidad
- ‚ö†Ô∏è NO expongas el puerto 3000 a internet sin autenticaci√≥n
- üí° Para producci√≥n: Agreg√° autenticaci√≥n JWT o OAuth

---

**Fecha**: Febrero 2026  
**Versi√≥n**: 1.0  
**Proyecto**: Simulador de Pacientes Virtuales
